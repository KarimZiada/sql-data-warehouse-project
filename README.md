# Data Warehouse & Analytics Project

Welcome to the **Data Warehouse & Analytics Project** repository! ğŸš€  
This project showcases an end-to-end data warehousing and analytics solution, covering everything from data ingestion and transformation to dimensional modeling and analytical reporting.

Built as a portfolio project, it demonstrates industry-standard data engineering practices, including:

- Scalable data modeling  
- Rigorous data quality validation  
- Dimension and fact table design  
- Analytical querying and reporting  

A core highlight of this implementation is the use of the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**, enabling a structured, incremental, and reliable data transformation workflow.

This repository provides a practical example of how modern data platforms process raw operational data into high-quality, analytics-ready datasets that drive actionable insights.

----------------


## ğŸ—ï¸ Data Architecture

The data architecture for this project follows the **Medallion Architecture** consisting of **Bronze**, **Silver**, and **Gold** layers.

![Data Architecture Diagram](docs/data_architecture.png)

### **1. Bronze Layer**
Stores raw data exactly as received from the source systems.  
Data is ingested from **CSV files** into the **SQL Server** database without modification.

### **2. Silver Layer**
Performs **cleaning, standardization, and normalization** to improve data quality  
and ensure consistent structure across all datasets.

### **3. Gold Layer**
Contains **business-ready, analytics-optimized data** modeled into a **star schema**,  
used for reporting, dashboards, and advanced analytics.

----------------------

## ğŸ“– Project Overview

This project showcases the end-to-end development of a modern **Data Warehouse** and analytics ecosystem. It covers the full lifecycle of data engineering, from ingestion to business-ready insights.

### ğŸ”§ Key Components

- **Data Architecture:** Implementation of the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) to ensure structured, scalable, and high-quality data processing.
- **ETL Pipelines:** Extraction, transformation, and loading of data from CSV sources into a SQL Serverâ€“based warehouse.
- **Data Modeling:** Design of optimized **dimensional models** (fact and dimension tables) suitable for analytical workloads.
- **Analytics & Reporting:** Creation of SQL-driven analyses and dashboards that deliver meaningful business insights.

### ğŸ¯ Who This Project Is For

This repository is designed as a strong portfolio piece, showcasing **my data engineering skills for job opportunities** and providing value to professionals and learners interested in:

- **SQL Development**
- **Data Architecture**
- **Data Engineering**
- **ETL Pipeline Development**
- **Data Modeling**
- **Data Analytics**

It serves as a practical, industry-aligned demonstration of how modern data platforms transform raw data into reliable, analytics-ready insights.

--------------

## ğŸ› ï¸ Important Links & Tools

Everything is **100% Free** to use!

- **Datasets:** Access the project CSV files  
  ğŸ‘‰ [Project Datasets](docs/datasets)

- **SQL Server (Docker Image):** Pull and run SQL Server using Docker  
  ğŸ‘‰ `docker pull mcr.microsoft.com/azure-sql-edge`

- **Visual Studio Code:** Download the free code editor for SQL, DevOps, and scripting  
  ğŸ‘‰ [Download VS Code](https://code.visualstudio.com/)

- **Git Repository:** Create and manage your GitHub repo for version control  
  ğŸ‘‰ [GitHub](https://github.com/)

- **Draw.io:** Create architecture diagrams, data models, and flowcharts  
  ğŸ‘‰ [Draw.io](https://app.diagrams.net/)

- **Notion Template:** Access the project planning and documentation template  
  ğŸ‘‰ [Project Template on Notion](https://www.notion.so)

-------------------------

## ğŸš€ Project Requirements

### Building the Data Warehouse (Data Engineering)

#### ğŸ¯ Objective
Design and implement a modern data warehouse in SQL Server to centralize sales information and support analytical reporting and data-driven decision-making.

#### ğŸ“Œ Specifications

- **Data Sources:** Load and integrate data from two operational systemsâ€”ERP and CRMâ€”provided as CSV files.
- **Data Quality:** Apply cleaning, standardization, and validation steps to address inconsistencies before analysis.
- **Integration:** Merge both datasets into a unified, analytics-friendly data model optimized for querying.
- **Scope:** Process only the most recent snapshot of data; historical tracking is not required for this project.
- **Documentation:** Deliver clear and accessible documentation of the data model for business users and analytics teams.


-------------------------

## ğŸŒŸ About Me

Hi! I'm **Karim Ziada**, a passionate Data Engineer and aspiring technologist dedicated to building modern data solutions.  
This project showcases my hands-on experience with data architecture, SQL development, ETL pipelines, and analyticsâ€”demonstrating the skills Iâ€™m developing for future **data engineering job opportunities**.



